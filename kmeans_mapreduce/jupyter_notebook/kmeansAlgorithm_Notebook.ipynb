{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# k-means algorithm implementation on Hadoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data points generation\n",
    "### createDataPoints.py\n",
    "\n",
    "The initial task of the project is to generate a set of more than one million data points to be used later as input for the k-means clustering algorithm. Using this python script three isotropic Gaussian blobs for clustering are generated. More specifically, the centers are the following data points [25, 25], [-1, -1], [-25, -25]. Additionally, the data points are presented visually with the use of a\n",
    "scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"createDataPoints.py: Generate data points for clustering.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "__author__ = \"Stratos Gounidellis, Lamprini Koutsokera\"\n",
    "__copyright__ = \"Copyright 2017, BDSMasters\"\n",
    "\n",
    "\n",
    "class DataGenerator():\n",
    "\n",
    "    def generateData(self, points, dataFile):\n",
    "        \"\"\"Generate the input data points.\n",
    "\n",
    "        :param self: An instance of the class DataGenerator.\n",
    "        :param points: The number of data points to be generated.\n",
    "        :param dataFile: The file to save the data points.\n",
    "        \"\"\"\n",
    "        centers = [[25, 25], [-1, -1], [-25, -25]]\n",
    "        X, labels_true = make_blobs(n_samples=long(points),\n",
    "                                    centers=centers, cluster_std=3.5,\n",
    "                                    n_features=2)\n",
    "\n",
    "        df = pd.DataFrame(X)\n",
    "        df.to_csv(dataFile, header=False, index=False, sep=\" \")\n",
    "\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=labels_true)\n",
    "        directory = \"../images\"\n",
    "        if not os.path.isdir(directory):\n",
    "            os.makedirs(directory)\n",
    "        plt.savefig(\"../images/data_points.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"dataFile\", type=str,\n",
    "                        help=\"File to save the generated data points.\")\n",
    "\n",
    "    parser.add_argument(\"points\", type=int,\n",
    "                        help=\"Number of data points to create.\")\n",
    "    args = parser.parse_args()\n",
    "    instanceDataGenerator = DataGenerator()\n",
    "    instanceDataGenerator.generateData(args.points, args.dataFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/data_points.png\", width=\"400\", height=\"400\", alt=\"Generated data points\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of clusters\n",
    "### plotSilhouetteScore.py\n",
    "\n",
    "The silhouette score constitutes a useful criterion for determining the proper number of clusters and it was firstly suggested by Peter J. Rousseeuw. The silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. A silhouette close to 1 implies the datum is in an appropriate cluster, while a silhouette close to -1 implies the datum is in the wrong cluster.\n",
    "\n",
    "The following python script calculates the silhouette score for different numbers of clusters ranging from 2 to 6. With this script not only the average silhouette score of each cluster is visualized but also the thickness (i.e. the number of data points) of each cluster. The number of clusters which leads to clusters of more or less similar thickness and silhouette score above the average could be the optimal number of clusters for the k-means algorithm.\n",
    "\n",
    "As expected creating three clusters is the optimal solution in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plotSilhouetteScore.py: Selecting the number of clusters with\n",
    "                     silhouette analysis on k-means clustering.\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the\n",
    "resulting clusters. The silhouette plot displays a measure of how close each\n",
    "point in one cluster is to points in the neighboring clusters and thus provides\n",
    "a way to assess parameters like number of clusters visually. This measure has a\n",
    "range of [-1, 1].\n",
    "\n",
    "Silhouette coefficients (as these values are referred to as) near +1 indicate\n",
    "that the sample is far away from the neighboring clusters. A value of 0\n",
    "indicates that the sample is on or very close to the decision boundary between\n",
    "two neighboring clusters and negative values indicate that those samples might\n",
    "have been assigned to the wrong cluster.\n",
    "\n",
    "Source:\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from kmeans import KmeansRunner\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "__author__ = \"Scikit-Learn\"\n",
    "\n",
    "\n",
    "class SilhouetteScore():\n",
    "\n",
    "    def calculateSilhouetteScore(self, dataFile):\n",
    "        \"\"\"Calculate the silhouette score for different numbers of clusters.\n",
    "\n",
    "        :param self: An instance of the class SilhouetteScore.\n",
    "        :param dataFile: An array with the input data points.\n",
    "        :return: A list with the names of the image files created.\n",
    "        \"\"\"\n",
    "        instanceKmeans = KmeansRunner()\n",
    "        X = instanceKmeans.retrieveData(dataFile)\n",
    "        if (X.shape[0] > 10000):\n",
    "            size = round(X.shape[0] * 0.001)\n",
    "            idx = np.random.randint(X.shape[0], size=size)\n",
    "            subset = X[idx, :]\n",
    "            X = subset\n",
    "        range_n_clusters = [2, 3, 4, 5, 6]\n",
    "        list_images = []\n",
    "\n",
    "        for n_clusters in range_n_clusters:\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "            fig.set_size_inches(18, 7)\n",
    "\n",
    "            ax1.set_xlim([-0.1, 1])\n",
    "\n",
    "            ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "            cluster_labels = clusterer.fit_predict(np.array(X))\n",
    "\n",
    "            silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "            print(\"For n_clusters =\", n_clusters,\n",
    "                  \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "            sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "            y_lower = 10\n",
    "            for i in range(n_clusters):\n",
    "\n",
    "                ith_cluster_silhouette_values = \\\n",
    "                    sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "                ith_cluster_silhouette_values.sort()\n",
    "\n",
    "                size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "                y_upper = y_lower + size_cluster_i\n",
    "\n",
    "                color = cm.spectral(float(i) / n_clusters)\n",
    "                ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                                  0, ith_cluster_silhouette_values,\n",
    "                                  facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "                ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "                y_lower = y_upper + 10\n",
    "\n",
    "            ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "            ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "            ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "            ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "            ax1.set_yticks([])\n",
    "            ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "            colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "            ax2.scatter(X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7,\n",
    "                        c=colors)\n",
    "\n",
    "            centers = clusterer.cluster_centers_\n",
    "            ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                        marker=\"o\", c=\"white\", alpha=1, s=200)\n",
    "\n",
    "            for i, c in enumerate(centers):\n",
    "                ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50)\n",
    "\n",
    "            ax2.set_title(\"The visualization of the clustered data.\")\n",
    "            ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "            ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "            plt.suptitle((\"Silhouette analysis for k-means\"\n",
    "                          \"clustering on sample data \"\n",
    "                          \"with n_clusters = %d\" % n_clusters),\n",
    "                         fontsize=14, fontweight=\"bold\")\n",
    "            fig.savefig(\"cluster_\" + str(n_clusters) + \".png\")\n",
    "            list_images.append(\"cluster_\" + str(n_clusters) + \".png\")\n",
    "        return list_images\n",
    "\n",
    "    def silhouetteScoretoPNG(self, list_images):\n",
    "        \"\"\"Save the results of the plots in asingle image file.\n",
    "\n",
    "        :param self: An instance of the class SilhouetteScore.\n",
    "        :param list_images: A list with the name of the image files created.\n",
    "        \"\"\"\n",
    "        clusterImages = [PIL.Image.open(i) for i in list_images]\n",
    "        minSize = sorted([(np.sum(i.size), i.size)\n",
    "                          for i in clusterImages])[0][1]\n",
    "\n",
    "        imagesCombination = np.vstack((np.asarray(i.resize(minSize))\n",
    "                                       for i in clusterImages))\n",
    "        imagesCombination = PIL.Image.fromarray(imagesCombination)\n",
    "        directory = \"../images\"\n",
    "        if not os.path.isdir(directory):\n",
    "            os.makedirs(directory)\n",
    "        imagesCombination.save(\"../images/clustersScore.png\")\n",
    "        for image in list_images:\n",
    "            os.remove(image)\n",
    "        print (\"The silhouette score for the number of\"\n",
    "               \" clusters ranging from 2 \"\n",
    "               \"to 6 has been saved in the file clustersScore.png!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"dataFile\", type=str,\n",
    "                        help=\"File to retrieve the generated data points.\")\n",
    "    args = parser.parse_args()\n",
    "    instanceSilhouetteScore = SilhouetteScore()\n",
    "    images = instanceSilhouetteScore.calculateSilhouetteScore(args.dataFile)\n",
    "    instanceSilhouetteScore.silhouetteScoretoPNG(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"../images/clustersScore.png\", height=\"600\", width=\"750\", alt=\"Clusters: Silhouette Score\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This python script calls the k-means algorithm implemented on hadoop. However, before implementing k-means the initial centroids are computed using the k-means++ algorithm proposed in 2007 by Arthur and Vassilvitskii. The steps of the algorithm are the following:\n",
    "\n",
    "<img src=\"../images/kmeans++.png\", height=\"350\", width=\"350\", alt=\"k-means++ algorithm\">\n",
    "\n",
    "After determining the initial centroids, k-means algorithm is called in order to detetermine the new centroids of the clusters and the results are saved as an image file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"kmeans.py: Run the k-means algorithm.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "\n",
    "__author__ = \"Stratos Gounidellis, Lamprini Koutsokera\"\n",
    "__copyright__ = \"Copyright 2017, BDSMasters\"\n",
    "\n",
    "\n",
    "class KmeansRunner():\n",
    "\n",
    "    def retrieveData(self, file):\n",
    "        \"\"\"Retrieve the data points from the input file.\n",
    "\n",
    "        :param self: An instance of the class KmeansRunner.\n",
    "        :param file: A file with the input data.\n",
    "        :return: An array with the input data points.\n",
    "        \"\"\"\n",
    "        df_points = pd.read_csv(file, header=None, names=[\"x\", \"y\"], sep=\" \")\n",
    "        if (len(df_points.index) < 1):\n",
    "            raise Exception(\"The input file is empty!\")\n",
    "        data = [tuple(row) for row in df_points.values]\n",
    "        points = np.array([data_point for data_point in data])\n",
    "        return points\n",
    "\n",
    "    def initialCentroids(self, file, nclusters):\n",
    "        \"\"\"Calculate the initial centroids to be used by the k-means\n",
    "            clustering algorithm.\n",
    "\n",
    "        :param self: An instance of the class KmeansRunner.\n",
    "        :param file: A file with the input data.\n",
    "        :param nclusters: The number of clusters.\n",
    "        :return: A list with the initial centroids.\n",
    "        \"\"\"\n",
    "        points = self.retrieveData(file)\n",
    "        initial_centroids = [list(random.choice(points))]\n",
    "        dist = []\n",
    "        if nclusters < 2:\n",
    "            raise Exception(\"Error the number of clusters should be\" +\n",
    "                            \" greater than or equal to 2!\")\n",
    "        for i in range(2, nclusters + 1):\n",
    "            dist.append([np.linalg.norm(np.array(point) -\n",
    "                        initial_centroids[i - 2])**2 for point in points])\n",
    "            min_dist = dist[0]\n",
    "            if (len(dist) > 1):\n",
    "                min_dist = np.minimum(\n",
    "                    min_dist, (dist[index] for index in range(1, len(dist))))\n",
    "\n",
    "            sumValues = sum(min_dist)\n",
    "            probabilities = [float(value) / sumValues for value in min_dist]\n",
    "            cumulative = np.cumsum(probabilities)\n",
    "\n",
    "            random_index = random.random()\n",
    "            index = np.where(cumulative >= random_index)[0][0]\n",
    "            initial_centroids.append(list(points[index]))\n",
    "\n",
    "        return initial_centroids\n",
    "\n",
    "    def retrieveCentroids(self, file):\n",
    "        \"\"\"Retrieve the centroids coordinated from the centroids file.\n",
    "\n",
    "        :param self: An instance of the class KmeansRunner.\n",
    "        :param file: A file with the centroids.\n",
    "        :return: A list with the centroids.\n",
    "        \"\"\"\n",
    "        with open(file, \"r\") as inputFile:\n",
    "            output_data = inputFile.readlines()\n",
    "\n",
    "        centroids = []\n",
    "        for point in output_data:\n",
    "            p = re.search(\"\\[(.*?)\\]\", point).group()\n",
    "            p = p.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            p.strip()\n",
    "            axisx, axisy = p.split(\",\")\n",
    "            axisx = float(axisx)\n",
    "            axisy = float(axisy)\n",
    "            point_list = [axisx, axisy]\n",
    "            centroids.append(point_list)\n",
    "        return centroids\n",
    "\n",
    "    def retrieveLabels(self, dataFile, centroidsFile):\n",
    "        \"\"\"Retrieve the labels of the imput data points.\n",
    "\n",
    "        :param self: An instance of the class KmeansRunner.\n",
    "        :param dataFile: A file with the input data points.\n",
    "        :param centroidsFile: A file with the centroids.\n",
    "        :return: A list with the labels.\n",
    "        \"\"\"\n",
    "        data_points = self.retrieveData(dataFile)\n",
    "        centroids = self.retrieveCentroids(centroidsFile)\n",
    "        labels = []\n",
    "        for data_point in data_points:\n",
    "            distances = [np.linalg.norm(data_point - centroid)\n",
    "                         for centroid in centroids]\n",
    "            cluster = np.argmin(distances)\n",
    "            labels.append(int(cluster))\n",
    "        return labels\n",
    "\n",
    "    def writeCentroids(self, centroids, file):\n",
    "        \"\"\"Write centroids to a file.\n",
    "\n",
    "        :param self: An instance of the class KmeansRunner.\n",
    "        :param centroids: A list with the centroids.\n",
    "        :param file: A file to write the centroids.\n",
    "        \"\"\"\n",
    "        f = open(CENTROIDS_FILE, \"w+\")\n",
    "        for item in centroids:\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "        f.close()\n",
    "\n",
    "    def plotClusters(self, data_points, centroids, labels):\n",
    "        \"\"\"Plot the clusters with the centroids and save the plot as an image.\n",
    "\n",
    "        :param self: An instance of the class KmeansRunner.\n",
    "        :param data_points: An array with the input data points.\n",
    "        :param centroids: A list with the centroids.\n",
    "        :param labels: The labels of the input data points.\n",
    "        \"\"\"\n",
    "        plt.scatter(data_points[:, 0], data_points[:, 1], c=labels)\n",
    "        for i in range(len(centroids)):\n",
    "            label = \"Centroid \" + str(i)\n",
    "            colors = [\"red\", \"green\", \"blue\"]\n",
    "            plt.scatter(centroids[i][0], centroids[i][1], s=50,\n",
    "                        c=colors[i], label=label)\n",
    "        plt.legend(loc=\"best\", fancybox=True)\n",
    "        fig = plt.gcf()\n",
    "        plt.show()\n",
    "        directory = \"../images\"\n",
    "        if not os.path.isdir(directory):\n",
    "            os.makedirs(directory)\n",
    "        fig.savefig(\"../images/clusters.png\")\n",
    "\n",
    "\n",
    "CENTROIDS_FILE = \"centroids.txt\"\n",
    "OUTPUT_FILE = \"output.txt\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse\n",
    "    parser = argparse.ArgumentParser(description=\"k-means algorithm\"\n",
    "                                     \" implementation on Hadoop\",\n",
    "                                     epilog=\"Go ahead and try it!\")\n",
    "    parser.add_argument(\"inputFile\", type=str,\n",
    "                        help=\"Input data points for the clustering algorithm.\")\n",
    "    parser.add_argument(\"centroids\", type=int,\n",
    "                        help=\"Number of clusters.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data = args.inputFile\n",
    "    k = args.centroids\n",
    "    instanceKmeans = KmeansRunner()\n",
    "    centroids = instanceKmeans.initialCentroids(data, int(k))\n",
    "    instanceKmeans.writeCentroids(centroids, CENTROIDS_FILE)\n",
    "\n",
    "    outputFile = open(OUTPUT_FILE, \"w+\")\n",
    "    outputFile.close()\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        print \"k-means iteration #%i\" % i\n",
    "\n",
    "        command = \"python kmeansAlgorithm.py < \" \\\n",
    "                  + data + \" --k=\" \\\n",
    "                  + str(k) + \" --centroids=\" \\\n",
    "                  + CENTROIDS_FILE + \" > \" + OUTPUT_FILE \\\n",
    "                  + \" -r hadoop\"\n",
    "        os.popen(command)\n",
    "\n",
    "        new_centroids = instanceKmeans.retrieveCentroids(OUTPUT_FILE)\n",
    "\n",
    "        if sorted(centroids) != sorted(new_centroids):\n",
    "            centroids = new_centroids\n",
    "            instanceKmeans.writeCentroids(centroids, CENTROIDS_FILE)\n",
    "        else:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    os.remove(OUTPUT_FILE)\n",
    "    labels = instanceKmeans.retrieveLabels(data, CENTROIDS_FILE)\n",
    "    labelsFile = open(\"labels.txt\", \"w+\")\n",
    "    for label in labels:\n",
    "        labelsFile.write(\"%s\\n\" % str(label))\n",
    "    labelsFile.close()\n",
    "    data_points = instanceKmeans.retrieveData(data)\n",
    "    instanceKmeans.plotClusters(data_points, centroids, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeansAlgorithm.py\n",
    "\n",
    "In order to implement k-means algorithm on hadoop mrjob is used. Mrjob is a python package, which allows to write multi-step MapReduce jobs in pure Python and run them on a hadoop cluster. In our case mrjob run on a single-node cluster. (The script can also be run locally by commenting the argument \"-r hadoop\".)\n",
    "\n",
    "#### k-means algorithm\n",
    "1. Define the number of clusters, k.\n",
    "2. Select k data points as initial centroids. \n",
    "3. Assign each data object to the closest cluster centroid.\n",
    "4. Recalculate the clustersâ€™ centroids.\n",
    "5.  If the centroids remain unchanged the algorithm terminates. Otherwise, the steps are repeated from Step 2.\n",
    "\n",
    "#### k-means algorithm - MapReduce\n",
    "1. The mapper function returns each data point and the cluster, to which it belongs.\n",
    "2. The combiner function returns partial sums of batches of data points belonging to the same cluster.  \n",
    "3. The reducer returns the new centroids of each cluster.\n",
    "4. If the centroids remain unchanged the algorithm terminates. Otherwise, the steps are repeated from the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"kmeansAlgorithm.py: Implement the k-means clustering\n",
    "    algorithm on the input data.\"\"\"\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "__author__ = \"Stratos Gounidellis, Lamprini Koutsokera\"\n",
    "__copyright__ = \"Copyright 2017, BDSMasters\"\n",
    "\n",
    "\n",
    "class KmeansAlgorithm(MRJob):\n",
    "    def configure_options(self):\n",
    "        \"\"\"Set the arguments for the class KmeansAlgorithm.\n",
    "\n",
    "        :param self: A instance of the class KmeansAlgorithm.\n",
    "        \"\"\"\n",
    "        super(KmeansAlgorithm, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            \"--k\", type=\"int\", help=\"Number of clusters.\")\n",
    "        self.add_file_option(\"--centroids\")\n",
    "\n",
    "    def retrieveCentroids(self, file):\n",
    "        \"\"\"Retrieve the centroids coordinated from the centroids file.\n",
    "\n",
    "        :param self: An instance of the class KmeansAlgorithm.\n",
    "        :param file: A file with the centroids.\n",
    "        :return: A list with the centroids.\n",
    "        \"\"\"\n",
    "        with open(file, \"r\") as inputFile:\n",
    "            output_data = inputFile.readlines()\n",
    "\n",
    "        centroids = []\n",
    "        for point in output_data:\n",
    "            p = re.search(\"\\[(.*?)\\]\", point).group()\n",
    "            p = p.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            p.strip()\n",
    "            axisx, axisy = p.split(\",\")\n",
    "            axisx = float(axisx)\n",
    "            axisy = float(axisy)\n",
    "            point_list = [axisx, axisy]\n",
    "            centroids.append(point_list)\n",
    "        return centroids\n",
    "\n",
    "    def assignPointtoCluster(self, _, line):\n",
    "        \"\"\"Assign each point to its closest cluster - Mapper Function.\n",
    "\n",
    "        :param self: An instance of the class KmeansAlgorithm.\n",
    "        :param line: A line from the input data, with data points in\n",
    "            the form [axisx axisy]\n",
    "        :yield: The identifier of a cluster and a point belonging to it.\n",
    "        \"\"\"\n",
    "        axisx, axisy = line.split()\n",
    "        data_point = np.array([float(axisx), float(axisy)])\n",
    "        centroids = self.retrieveCentroids(self.options.centroids)\n",
    "        distances = [np.linalg.norm(data_point - centroid)\n",
    "                     for centroid in centroids]\n",
    "        cluster = np.argmin(distances)\n",
    "        yield int(cluster), data_point.tolist()\n",
    "\n",
    "    def calculatePartialSum(self, cluster, data_points):\n",
    "        \"\"\"Calculate the partial sum of the data points belonging to\n",
    "            each cluster - Combiner Function.\n",
    "\n",
    "        :param self: An instance of the class KmeansAlgorithm.\n",
    "        :param cluster: An identifier for each cluster.\n",
    "        :param data_points: A list of points belonging to each cluster.\n",
    "        :yield: The identifier of a cluster, the partial sum of its\n",
    "            data points and their number.\n",
    "        \"\"\"\n",
    "        sum_points = np.array(data_points.next())\n",
    "        counter = 1\n",
    "        for data_point in data_points:\n",
    "            sum_points += data_point\n",
    "            counter += 1\n",
    "        yield cluster, (sum_points.tolist(), counter)\n",
    "\n",
    "    def calculateNewCentroids(self, cluster, partial_sums):\n",
    "        \"\"\"Calculate the new centroids of the clusters - Reduce Function.\n",
    "\n",
    "        :param self: An instance of the class KmeansAlgorithm.\n",
    "        :param cluster: An identifier for each cluster.\n",
    "        :param partial_sums: A list with the partial sum of the\n",
    "            data points of a cluster and their number.\n",
    "        :yield: The identifier of a cluster and its new centroid.\n",
    "        \"\"\"\n",
    "        total_sum, total_counter = partial_sums.next()\n",
    "        total_sum = np.array(total_sum)\n",
    "        for partial_sum, counter in partial_sums:\n",
    "            total_sum += partial_sum\n",
    "            total_counter += counter\n",
    "        yield cluster, (total_sum / total_counter).tolist()\n",
    "\n",
    "    def steps(self):\n",
    "        \"\"\"Set the steps of the MRJob.\n",
    "\n",
    "        :param self: An instance of the class KmeansAlgorithm.\n",
    "\n",
    "        :return: a list of steps constructed with MRStep().\n",
    "        \"\"\"\n",
    "        return [MRStep(mapper=self.assignPointtoCluster,\n",
    "                combiner=self.calculatePartialSum,\n",
    "                reducer=self.calculateNewCentroids)]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    KmeansAlgorithm.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load centroids.txt\n",
    "[-25.0435171775063, -25.312381535798494]\n",
    "[-0.8285630901724736, -0.9468515624217155]\n",
    "[24.872054053869434, 24.90708854217947]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/clusters.png\", height=\"400\", width=\"400\", alt=\"Clusters & Centroids\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from createDataPoints import DataGenerator\n",
    "from kmeans import KmeansRunner\n",
    "\n",
    "__author__ = \"Stratos Gounidellis, Lamprini Koutsokera\"\n",
    "__copyright__ = \"Copyright 2017, BDSMasters\"\n",
    "\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def test_dataPoints(self):\n",
    "        instanceData = DataGenerator()\n",
    "        fname = \"test.txt\"\n",
    "        instanceData.generateData(100, fname)\n",
    "        with open(fname) as f:\n",
    "            for i, l in enumerate(f):\n",
    "                pass\n",
    "        i + 1\n",
    "        self.assertEqual(100, i+1)\n",
    "\n",
    "    def test_exceptionClustersNumber(self):\n",
    "        fname = \"test.txt\"\n",
    "        instanceKmeans = KmeansRunner()\n",
    "        with self.assertRaises(Exception) as context:\n",
    "            instanceKmeans.initialCentroids(fname, 1)\n",
    "        self.assertIn(\"Error the number of clusters should be greater\" +\n",
    "                      \"than or equal to 2!\", \"\".join(context.exception))\n",
    "\n",
    "    def test_fileLength(self):\n",
    "        fname = \"test.txt\"\n",
    "        instanceKmeans = KmeansRunner()\n",
    "        testFile = open(fname, \"w+\")\n",
    "        testFile.close()\n",
    "        with self.assertRaises(Exception) as context:\n",
    "            instanceKmeans.retrieveData(fname)\n",
    "        self.assertIn(\"The input file is empty!\", \"\".join(context.exception))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n"
   ]
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "MarkdownCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
